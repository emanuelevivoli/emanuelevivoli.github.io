<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Emanuele Vivoli</title>

    <meta name="author" content="Emanuele Vivoli">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    
    <script>
      document.addEventListener('DOMContentLoaded', function() {
        // Add click handlers for all toggle buttons
        document.querySelectorAll('.toggle-description').forEach(button => {
          button.addEventListener('click', function() {
            const description = this.nextElementSibling;
            description.classList.toggle('show');
            this.classList.toggle('collapsed');
          });
        });
      });
    </script>
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Emanuele Vivoli
                </p>
                <p>I am a PhD student jointly at <a href="https://www.cvc.uab.es">Computer Vision Center</a> (UAB, Barcelona) and <a href="https://www.micc.unifi.it">MICC</a> (UNIFI, Italy), where I work on vision and language, particularly on Comics/Manga, supervised by <a href="https://www.micc.unifi.it/bertini">Marco Bertini</a> and <a href="http://pages.cvc.uab.es/dimos">Dimosthenis Karatzas</a>.
                </p>
                <p>
                  In reverse order:
                  I started my PhD in November 2022 in Florence, and October 2023 in Barcelona.
                  Previously, I interned in 2022 at the <a href="https://www.cvc.uab.es">Computer Vision Center</a> (UAB, Barcelona), working in Multilingual Scene-Text VQA.
                  From 2021 to 2022 I worked as a researcher in the <a href="https://ai.dinfo.unifi.it/">AILab</a> (UNIFI, Italy), supervised by <a href="https://cercachi.unifi.it/p-doc2-2013-200006-M-3f2a3d2f3b3030-0.html">Simone Marinai</a>, working on Document and Table Recognition.
                  Finally, I interned for a research stay at <a href="https://www.ltu.se/om-universitetet/organisation/institutionen-for-system--och-rymdteknik/eislab">EISLAB</a> (Luleå Technical University) in 2019 supervised by <a href="https://www.ltu.se/en/staff/m/marcus-liwicki">Marcus Liwicki</a>, working on EEG and RNNs.
                </p>
                <p>
                  I have published in conferences such as NeurIPS, ECCV, BMVC, ICDAR, ICPR, IRCLD, ACM DocEng. I have served as a reviewer for NeurIPS, CVPR, ECCV, ICCV, BMVC, ACM Multimedia, ICDAR, and IJDAR.
                </p>
                <p>
                  I have also worked on Landmine detection for saving lives and humanitarian demining. Check my <a href="https://tinyurl.com/evivoli-Gscholar">Google Scholar</a> for more info.
                </p>
                <p style="text-align:center">
                  <a href="mailto:emanuele.vivoli@unifi.it">Email</a> &nbsp;/&nbsp;
                  <a href="data/EmanueleVivoli-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="data/EmanueleVivoli-bio.txt">Bio</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=BCzPjawAAAAJ&hl">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/emanuelevivoli">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/emanuelevivoli">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/EmanueleVivoli-new.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/EmanueleVivoli-new.png" class="hoverZoomLink profile-image"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in Vision and Language, more specifically in Comics/Manga. My works are mainly about Comics Understanding, bridging the gap between the Comic medium and Vision and Language. Some papers are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
    <tr bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/25-ICCVw-VisionDocs.png' width="100%">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2507.10053">
          <span class="papertitle">CoSMo: A Multimodal Transformer for Page Stream Segmentation in Comic Books</span>
        </a>
        <br>
        <a href="https://www.linkedin.com/in/marc-serra-ortega">Marc Serra Ortega</a>,
        <strong>Emanuele Vivoli</strong>,
        <a href="https://scholar.google.com/citations?user=0VToXYcAAAAJ&hl=en">Artemis Llabrés</a>,
        <a href="https://scholar.google.com/citations?user=xASEtrUAAAAJ&hl=en">Dimosthenis Karatzas</a>
        <br>
        <em>ICCV (workshop) VisionDocs<em>, 2025
        <br><br>
        <span>
          <a href="https://github.com/mserra0/CoSMo-ComicsPSS" class="icon-link">
            <i class="fab fa-github"></i>
          </a>
          &nbsp;&nbsp;
          <a href="https://arxiv.org/abs/2507.10053" class="icon-link">
            <i class="fas fa-scroll"></i>
          </a>
        </span>
        <button class="toggle-description collapsed">description</button>
        <div class="paper-description">
          <p>
            In this work, we introduce CoSMo, a multimodal transformer for page stream segmentation in comic books that establishes a new state-of-the-art by leveraging visual and textual features to accurately segment comic book pages into reading streams, outperforming significantly larger vision-language models on a newly curated dataset of over 20k annotated pages.
          </p>
        </div>
      </td>
    </tr>
    <tr>
      <td colspan="2">
        <hr class="paper-separator">
      </td>
    </tr>
    <tr bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/25-ICDAR.png' width="100%">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2503.08561">
          <span class="papertitle">ComicsPAP: understanding comic strips by picking the correct panel</span>
        </a>
        <br>
        <strong>Emanuele Vivoli</strong>,
        <a href="https://scholar.google.com/citations?user=0VToXYcAAAAJ&hl=en">Artemis Llabrés</a>,
        <a href="https://scholar.google.com/citations?user=LXq3YYMAAAAJ&hl=en">Mohamed Ali Souibgui</a>,
        <a href="https://scholar.google.com/citations?user=SBm9ZpYAAAAJ&hl=en">Marco Bertini</a>,
        <a href="https://scholar.google.com/citations?user=ChmX8ogAAAAJ&hl=en">Ernest Valveny Llobet</a>,
        <a href="https://scholar.google.com/citations?user=xASEtrUAAAAJ&hl=en">Dimosthenis Karatzas</a>
        <br>
        <em>ICDAR</em>, 2025 &nbsp <font color="red"><strong>(oral)</strong></font>
        <br><br>
        <span>
          <a href="https://huggingface.co/collections/VLR-CVC/comics-pick-a-panel-67d41b505baf5761e2fadd07" class="icon-link">
            <i class="fas fa-globe"></i>
          </a>
          &nbsp;&nbsp;
          <a href="https://github.com/llabres/ComicsPAP" class="icon-link">
            <i class="fab fa-github"></i>
          </a>
          &nbsp;&nbsp;
          <a href="https://arxiv.org/abs/2503.08561" class="icon-link">
            <i class="fas fa-scroll"></i>
          </a>
        </span>
        <button class="toggle-description collapsed">description</button>
        <div class="paper-description">
          <p>
            In this work, I introduce ComicsPAP, a large-scale benchmark for comic strip understanding with over 100k samples organized into 5 subtasks. Through this Pick-a-Panel framework, I evaluate state-of-the-art MLLMs and demonstrate their limitations in capturing sequential and contextual dependencies, while also proposing adaptations that achieve better performance than models 10x larger.
          </p>
        </div>
      </td>
    </tr>
    <tr>
      <td colspan="2">
        <hr class="paper-separator">
      </td>
    </tr>
    
    
    <tr bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/24-ECCVw-AI4VA.png' width=100%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2409.16159">
          <span class="papertitle">ComiCap: A VLMs pipeline for dense captioning of Comic Panels</span>
        </a>
        <br>
        <strong>Emanuele Vivoli</strong>,
        <a href="https://scholar.google.com/citations?user=B7VHm9UAAAAJ&hl=en">Niccolò Biondi</a>,
        <a href="https://scholar.google.com/citations?user=SBm9ZpYAAAAJ&hl=en">Marco Bertini</a>,
        <a href="https://scholar.google.com/citations?user=xASEtrUAAAAJ&hl=en">Dimosthenis Karatzas</a>
        <br>
        <em>ECCV (workshop) AI4VA<em>, 2024
        <br><br>
        <span>
          <a href="https://emanuelevivoli.github.io/ComiCap" class="icon-link">
            <i class="fas fa-globe"></i>
          </a>
          &nbsp;&nbsp;
          <a href="https://github.com/emanuelevivoli/CoMix" class="icon-link">
            <i class="fab fa-github"></i>
          </a>
          &nbsp;&nbsp;
          <a href="https://arxiv.org/abs/2409.16159" class="icon-link">
            <i class="fas fa-scroll"></i>
          </a>
        </span>
        <button class="toggle-description collapsed">description</button>
        <div class="paper-description">
          <p>
            This work proposes a Vision-Language Model pipeline to generate dense, grounded captions for comic panels, outperforming task-specific models without additional training. I used it to annotate over 2 million panels, enhancing comic understanding.
          </p>
        </div>
      </td>
    </tr>
    <tr>
      <td colspan="2">
        <hr class="paper-separator">
      </td>
    </tr>



  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
        <img src='images/24-survey.png' width="160">
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://emanuelevivoli.github.io/comics-survey">
        <span class="papertitle">One missing piece in Vision and Language: A Survey on Comics Understanding</span>
      </a>
      <br>
			
      <strong>Emanuele Vivoli</strong>,
			<a href="https://dblp.org/pid/263/4653.html">Andrey Barsky</a>, 
			<a href="https://scholar.google.com/citations?user=LXq3YYMAAAAJ&hl=en">Mohammed Ali Soubgui</a>, 
			<a href="https://scholar.google.com/citations?user=0VToXYcAAAAJ&hl=en">Artemis Llabrés</a>, 
			<a href="https://scholar.google.com/citations?user=SBm9ZpYAAAAJ&hl=en">Marco Bertini</a>, 
			<a href="https://scholar.google.com/citations?user=xASEtrUAAAAJ&hl=en">Dimosthenis Karatzas</a>
      <br>
      <em>under review</em>, 2024
      <br><br>
      <a href="https://github.com/emanuelevivoli/awesome-comics-understanding" class="icon-link">
        <i class="fab fa-github"></i>
      </a>
      &nbsp;&nbsp;
      <a href="https://arxiv.org/abs/2409.09502" class="icon-link">
        <i class="fas fa-scroll"></i>
      </a>
      &nbsp;<button class="toggle-description collapsed">description</button>
      <div class="paper-description">
        <p>
          In this survey, I review comics understanding through the lens of vision-language models, introducing a new taxonomy, the Layer of Comics Understanding (LoCU), to redefine tasks in comics research, analyze datasets, and highlight challenges and future directions for AI applications in comics' unique visual-textual narratives.
        </p>
      </div>
    </td>
  </tr>
  <tr>
    <td colspan="2">
      <hr class="paper-separator">
    </td>
  </tr>


  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
        <img src='images/24-BMVC.png' width="160">
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://github.com/soumitri2001/GCPL">
		<span class="papertitle">Towards Generative Class Prompt Learning for Fine-grained Visual Recognition</span>
      </a>
      <br>
      <a href="https://scholar.google.com/citations?user=AyMx6O4AAAAJ&hl=en">Soumitri Chattopadhyay</a>,
	    <a href="https://scholar.google.com/citations?user=J-VlCNYAAAAJ&hl=en">Sanket Biswas</a>,
	    <strong>Emanuele Vivoli</strong>,
      <a href="https://scholar.google.com/citations?user=92pWl-AAAAAJ&hl=en">Josep Lladós</a>
      <br>
      <em>BMVC</em>, 2024 &nbsp <font color="red"><strong>(oral)</strong></font>
      <br><br>
      <a href="https://github.com/soumitri2001/GCPL" class="icon-link">
        <i class="fab fa-github"></i>
      </a>
      &nbsp;&nbsp;
      <a href="https://arxiv.org/abs/2409.01835" class="icon-link">
        <i class="fas fa-scroll"></i>
      </a>
      <button class="toggle-description collapsed">description</button>
      <div class="paper-description">
        <p>
          In this paper, I propose GCPL and CoMPLe to improve fine-grained categorization in vision-language models using generative modeling and contrastive learning, outperforming existing few-shot recognition methods.
        </p>
      </div>
    </td>
  </tr>
  <tr>
    <td colspan="2">
      <hr class="paper-separator">
    </td>
  </tr>


  <tr bgcolor="#ffffd0">
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
        <img src='images/24-NEURIPS.png' width="160">
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://emanuelevivoli.github.io/comix">
        <span class="papertitle">CoMix: A Comprehensive Benchmark for Multi-Task Comic Understanding</span>
      </a>
      <br>
      <strong>Emanuele Vivoli</strong>,
      <a href="https://scholar.google.com/citations?user=SBm9ZpYAAAAJ&hl=en">Marco Bertini</a>,
      <a href="https://scholar.google.com/citations?user=xASEtrUAAAAJ&hl=en">Dimosthenis Karatzas</a>
      <br>
      <em>NeurIPS (D&B)</em>, 2024
      <br><br>
      <a href="https://emanuelevivoli.github.io/CoMix-dataset" class="icon-link">
        <i class="fas fa-globe"></i>
      </a>
      &nbsp;&nbsp;
      <a href="https://github.com/emanuelevivoli/CoMix" class="icon-link">
        <i class="fab fa-github"></i>
      </a>
      &nbsp;&nbsp;
      <a href="https://arxiv.org/abs/2407.03550" class="icon-link">
        <i class="fas fa-scroll"></i>
      </a>
      <button class="toggle-description collapsed">description</button>
      <div class="paper-description">
        <p>
          CoMix is a multi-task comic analysis benchmark covering object detection, character identification, and multi-modal reasoning, using diverse datasets to balance styles beyond manga. I evaluate models in zero-shot and fine-tuning settings, revealing a gap between human and model performance, and set a new standard for comprehensive comic understanding.
        </p>
      </div>
    </td>
  </tr>
  <tr>
    <td colspan="2">
      <hr class="paper-separator">
    </td>
  </tr>


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/24-MANPU.png' width=100%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://github.com/emanuelevivoli/cdf">
          <span class="papertitle">Comics Datasets Framework: Mix of Comics datasets for detection benchmarking</span>
        </a>
        <br>
        <strong>Emanuele Vivoli</strong>,
        <a href="https://dblp.org/pid/382/4686.html">Irene Campaioli</a>,
        <a href="https://dblp.org/pid/382/5211.html">Mariateresa Nardoni</a>,
        <a href="https://scholar.google.com/citations?user=B7VHm9UAAAAJ&hl=en">Niccolò Biondi</a>,
        <a href="https://scholar.google.com/citations?user=SBm9ZpYAAAAJ&hl=en">Marco Bertini</a>,
        <a href="https://scholar.google.com/citations?user=xASEtrUAAAAJ&hl=en">Dimosthenis Karatzas</a>
        <br>
        <em>ICDAR (workshop) MANPU</em>, 2024 &nbsp <font color="red"><strong>(oral)</strong></font>
        <br>
        <br>
        <a href="https://emanuelevivoli.github.io/cdf" class="icon-link">
          <i class="fas fa-globe"></i>
        </a>
        &nbsp;&nbsp;
        <a href="https://github.com/emanuelevivoli/CoMix" class="icon-link">
          <i class="fab fa-github"></i>
        </a>
        &nbsp;&nbsp;
        <a href="https://arxiv.org/pdf/2407.03540" class="icon-link">
          <i class="fas fa-scroll"></i>
        </a>
        <button class="toggle-description collapsed">description</button>
        <div class="paper-description">
          <p>
            In this work, I introduce the Comics Datasets Framework, standardizing annotations and addressing manga overrepresentation with the Comics100 dataset. I benchmark detection architectures to tackle challenges like small datasets and inconsistent annotations, aiming to improve object detection and support more complex computational tasks in comics.
          </p>
        </div>
      </td>
    </tr>
	


    <tr bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/24-ICDAR.png' width=100%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://github.com/joanlafuente/ComicVT5">
          <span class="papertitle">Multimodal Transformer for Comics Text-Cloze</span>
        </a>
        <br>
				<strong>Emanuele Vivoli</strong>*,
				<a href="https://dblp.org/pid/372/0904.html">Joan Lafuente Baeza</a>*,
				<a href="https://scholar.google.com/citations?user=ChmX8ogAAAAJ&hl=en">Ernest Valveny Llobet</a>,
				<a href="https://scholar.google.com/citations?user=xASEtrUAAAAJ&hl=en">Dimosthenis Karatzas</a>	
        <br>
        <em>ICDAR</em>, 2024 &nbsp <font color="red"><strong>(oral)</strong></font>
        <br><br>
        <a href="https://github.com/joanlafuente/ComicVT5" class="icon-link">
          <i class="fab fa-github"></i>
        </a>
        &nbsp;&nbsp;
        <a href="https://arxiv.org/html/2403.03719v1" class="icon-link">
          <i class="fas fa-scroll"></i>
        </a>
        <button class="toggle-description collapsed">description</button>
        <div class="paper-description">
          <p>
            In this work, I introduce a Multimodal-LLM for a comics text-cloze task, improving accuracy by 10% over existing models through a domain-adapted visual encoder and new OCR annotations, and extend the task to a generative format.
          </p>
        </div>
      </td>
    </tr>
    <tr>
      <td colspan="2">
        <hr class="paper-separator">
      </td>
    </tr>


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/23-ACM-DocEng.png' width=100%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://dl.acm.org/doi/pdf/10.1145/3573128.3609351">
          <span class="papertitle">Deep-learning for dysgraphia detection in children handwritings</span>
        </a>
        <br>
        <a href="https://www.andreagemelli.me">Andrea Gemelli</a>*,
        <strong>Emanuele Vivoli</strong>*,
        <a href="https://scholar.google.com/citations?user=yLJ66pUAAAAJ&hl=en">Simone Marinai</a>,
        <a href="https://scholar.google.com/citations?user=zgA7UWoAAAAJ&hl=en">Tamara Zappaterra</a>
        <br>
        <em>ACM DocEng</em>, 2023 &nbsp <font color="red"><strong>(oral)</strong></font>
        <br><br>
        <a href="https://dl.acm.org/doi/pdf/10.1145/3573128.3609351" class="icon-link">
          <i class="fas fa-scroll"></i>
        </a>
        <button class="toggle-description collapsed">description</button>
        <div class="paper-description">
          <p>
            In this paper, I propose a smart pen and deep learning-based approach for early dysgraphia detection in children, offering a faster and more objective alternative to the traditional BHK test, validated through handwriting samples and expert interviews.
          </p>
        </div>
      </td>
    </tr>
    <tr>
      <td colspan="2">
        <hr class="paper-separator">
      </td>
    </tr>


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/23-IRCDL.png' width=100%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2302.01451">
          <span class="papertitle">CTE: A Dataset for Contextualized Table Extraction</span>
        </a>
        <br>
				<a href="https://www.andreagemelli.me">Andrea Gemelli</a>*,
				<strong>Emanuele Vivoli</strong>*,
        <a href="https://scholar.google.com/citations?user=yLJ66pUAAAAJ&hl=en">Simone Marinai</a>
        <br>
        <em>IRCDL</em>, 2023
        <br><br>
        <a href="https://github.com/AILab-UniFI/cte-dataset" class="icon-link">
          <i class="fab fa-github"></i>
        </a>
        &nbsp;&nbsp;
        <a href="https://arxiv.org/abs/2302.01451" class="icon-link">
          <i class="fas fa-scroll"></i>
        </a>
        <button class="toggle-description collapsed">description</button>
        <div class="paper-description">
          <p>
            In this paper, I introduce Contextualized Table Extraction (CTE), a task to extract and structure tables within their document context, supported by a new dataset of 75k annotated pages from scientific papers, combining data from PubTables-1M and PubLayNet.
          </p>
        </div>
      </td>
    </tr>
    <tr>
      <td colspan="2">
        <hr class="paper-separator">
      </td>
    </tr>



    <tr bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/22-ECCVw-TIE.png' width=100%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2209.06730">
          <span class="papertitle">MUST-VQA: MUltilingual Scene-Text VQA</span>
        </a>
        <br>
				
				<strong>Emanuele Vivoli</strong>,
        <a href="https://scholar.google.com/citations?user=izIufcIAAAAJ&hl=en">Ali Furkan Biten</a>,
        <a href="https://scholar.google.com/citations?user=Z3GrrmIAAAAJ&hl=en">Andres Mafla</a>,
				<a href="https://scholar.google.com/citations?user=xASEtrUAAAAJ&hl=en">Dimosthenis Karatzas</a>,
				<a href="https://scholar.google.com/citations?user=U5DQ99QAAAAJ&hl=en">Lluis Gomez</a>
        <br>
        <em>ECCV (workshop) Text in Everything</em>, 2022
        <br><br>
        <a href="https://github.com/emanuelevivoli/must-vqa" class="icon-link">
          <i class="fab fa-github"></i>
        </a>
        &nbsp;&nbsp;
        <a href="https://arxiv.org/abs/2209.06730" class="icon-link">
          <i class="fas fa-scroll"></i>
        </a>
        <button class="toggle-description collapsed">description</button>
        <div class="paper-description">
          <p>
            In this paper, I introduce a framework for Multilingual Scene Text Visual Question Answering (MUST-VQA) that handles new languages in a zero-shot setting, evaluates models in IID and zero-shot scenarios, and demonstrates the effectiveness of adapting multilingual language models for the STVQA task.
          </p>
        </div>
      </td>
    </tr>
    <tr>
      <td colspan="2">
        <hr class="paper-separator">
      </td>
    </tr>


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/22-ICPR.png' width="160">
        </div>
      </td>

      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://cat3d.github.io/">
			<span class="papertitle">Graph Neural Networks and Representation Embedding for Table Extraction in PDF Documents</span>
        </a>
        <br>
				<a href="https://www.andreagemelli.me">Andrea Gemelli</a>*,
        <strong>Emanuele Vivoli</strong>*,
        <a href="https://scholar.google.com/citations?user=yLJ66pUAAAAJ&hl=en">Simone Marinai</a>
        
        <br>
        <em>ICPR</em>, 2022
        <br><br>
        <a href="https://github.com/AILab-UniFI/GNN-TableExtraction" class="icon-link">
          <i class="fab fa-github"></i>
        </a>
        &nbsp;&nbsp;
        <a href="https://arxiv.org/abs/2208.11203" class="icon-link">
          <i class="fas fa-scroll"></i>
        </a>
        <button class="toggle-description collapsed">description</button>
        <div class="paper-description">
          <p>
				In this work, I address the problem of table extraction in scientific papers using Graph Neural Networks with enriched representation embeddings to improve distinction between tables, cells, and headers, evaluated on a dataset combining PubLayNet and PubTables-1M.
          </p>
        </div>
      </td>
    </tr>
    <tr>
      <td colspan="2">
        <hr class="paper-separator">
      </td>
    </tr>


          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Thanks to Jon Barron for the website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
</html>
