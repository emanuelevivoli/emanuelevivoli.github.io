<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Emanuele Vivoli</title>

    <meta name="author" content="Emanuele Vivoli">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Emanuele Vivoli
                </p>
                <p>Emanuele Vivoli is a PhD student jointly at <a href="https://www.cvc.uab.es">Computer Vision Center</a> (UAB, Barcelona) and <a href="https://www.micc.unifi.it">MICC</a> (UNIFI, Italy), where he works on vision and language, particularly on Comics/Manga, supervised by <a href="https://www.micc.unifi.it/bertini">Marco Bertini</a> and <a href="http://pages.cvc.uab.es/dimos">Dimosthenis Karatzas</a>.
                </p>
                <p>
                  In revers order: 
                  He started his PhD in November 2022 in Florence, and October 2023 in Barcelona.
                  Previously, he interned in 2022 the <a href="https://www.cvc.uab.es">Computer Vision Center</a> (UAB, Barcelona), working in Multilingua Scene-Text VQA.
                  From 2021 to 2022 he worked as researcher in the <a href="https://ai.dinfo.unifi.it/">AILab</a> (UNIFI, Italy), supervised by <a href="https://cercachi.unifi.it/p-doc2-2013-200006-M-3f2a3d2f3b3030-0.html">Simone Marinai</a>, working on Document and Table Recognition.
                  Finally, he interned for a research stay in <a href="https://www.ltu.se/om-universitetet/organisation/institutionen-for-system--och-rymdteknik/eislab">EISLAB</a> (Luleå Technique University) in 2019 supervised by <a href="https://www.ltu.se/en/staff/m/marcus-liwicki">Marcus Liwicki</a>, working on EEG and RNNs.
                </p>
                </p>
                <p>
                  He published in conferences such as NeurIPS, ECCV, BMVC, ICDAR, ICPR, IRCLD, ACM DocEng. He served as reviewer for ECCV, ICCV, CVPR, ACM Multimedia, ICDAR, and IJDAR.
                </p>
                <p>
                  He has worked also on Landmine detection for saving lifes and humanitarian demining. Check his <a href="https://tinyurl.com/evivoli-Gscholar">Google Scholar</a> for more info.
                </p>
                <p style="text-align:center">
                  <a href="mailto:emanuele.vivoli@unifi.it">Email</a> &nbsp;/&nbsp;
                  <a href="data/EmanueleVivoli-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="data/EmanueleVivoli-bio.txt">Bio</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=BCzPjawAAAAJ&hl">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/emanuelevivoli">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/emanuelevivoli">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/EmanueleVivoli-new.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/EmanueleVivoli-new.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in Vision and Languages, more specifically in Comics/Manga. My works are mainly about Comics Understanding, bridging the gap between Comic medium and Vision and Language. Some papers are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


    <tr bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/24-ECCVw-AI4VA.png' width=100%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2409.16159">
          <span class="papertitle">ComiCap: A VLMs pipeline for dense captioning of Comic Panels</span>
        </a>
        <br>
        <strong>Emanuele Vivoli</strong>,
        <a href="https://emanuelevivoli.github.io">Niccolò Biondi</a>,
        <a href="https://emanuelevivoli.github.io">Marco Bertini</a>,
        <a href="https://emanuelevivoli.github.io">Dimosthenis Karatzas</a>
        <br>
        <em>ECCV (workshop) AI4VA<em>, 2024
        <br>
        <a href="https://github.com/emanuelevivoli/comicap">github</a>
        /
        <a href="https://arxiv.org/abs/2409.16159">arXiv</a>
        <p></p>
        <p>
        This work proposes a Vision-Language Model pipeline to generate dense, grounded captions for comic panels, outperforming task-specific models without additional training and used to annotate over 2 million panels, enhancing comic understanding.
        </p>
      </td>
    </tr>



  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
        <img src='images/24-survey.png' width="160">
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://emanuelevivoli.github.io/comics-survey">
        <span class="papertitle">One missing piece in Vision and Language: A Survey on Comics Understanding</span>
      </a>
      <br>
			
      <strong>Emanuele Vivoli</strong>,
			<a href="https://emanuelevivoli.github.io">Andrey Barsky</a>, 
			<a href="https://emanuelevivoli.github.io">Mohammed Ali Soubgui</a>, 
			<a href="https://emanuelevivoli.github.io">Artemis Llabrés</a>, 
			<a href="https://emanuelevivoli.github.io">Marco Bertini</a>, 
			<a href="https://emanuelevivoli.github.io">Dimosthenis Karatzas</a>
      <br>
      <em>under review</em>, 2024
      <br>
      <a href="https://emanuelevivoli.github.io/comics-survey">project page</a>
      /
      <a href="https://github.com/emanuelevivoli/awesome-comics-understanding">github</a>
      /
      <a href="https://arxiv.org/abs/2409.09502">arXiv</a>
      <p></p>
      <p>
      This survey reviews comics understanding through the lens of vision-language models, introducing a new taxonomy, the Layer of Comics Understanding (LoCU), to redefine tasks in comics research, analyze datasets, and highlight challenges and future directions for AI applications in comics' unique visual-textual narratives.
      </p>
    </td>
  </tr>


  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
        <img src='images/24-BMVC.png' width="160">
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://github.com/soumitri2001/GCPL">
		<span class="papertitle">Towards Generative Class Prompt Learning for Fine-grained Visual Recognition</span>
      </a>
      <br>
      <a href="https://emanuelevivoli.github.io">Soumitri Chattopadhyay</a>,
	    <a href="https://emanuelevivoli.github.io">Sanket Biswas</a>,
	    <strong>Emanuele Vivoli</strong>,
      <a href="https://emanuelevivoli.github.io">Josep Lladós</a>
      <br>
      <em>BMVC</em>, 2024 &nbsp <font color="red"><strong>(oral)</strong></font>
      <br>
      <a href="https://github.com/soumitri2001/GCPL">project page</a>
      /
      <a href="https://arxiv.org/abs/2409.01835">arXiv</a>
      <p></p>
      <p>
      This paper proposes GCPL and CoMPLe to improve fine-grained categorization in vision-language models using generative modeling and contrastive learning, outperforming existing few-shot recognition methods.
      </p>
    </td>
  </tr>


  <tr bgcolor="#ffffd0">
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
        <img src='images/24-NEURIPS.png' width="160">
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://emanuelevivoli.github.io/comix">
        <span class="papertitle">CoMix: A Comprehensive Benchmark for Multi-Task Comic Understanding</span>
      </a>
      <br>
      <strong>Emanuele Vivoli</strong>,
      <a href="https://emanuelevivoli.github.io">Marco Bertini</a>,
      <a href="https://emanuelevivoli.github.io">Dimosthenis Karatzas</a>
      <br>
      <em>NeurIPS (D&B)</em>, 2024
      <br>
      <a href="https://emanuelevivoli.github.io/comix">project page</a>
      /
      <a href="https://arxiv.org/abs/2407.03550">arXiv</a>
      <p></p>
      <p>
      CoMix is a multi-task comic analysis benchmark covering object detection, character identification, and multi-modal reasoning, using diverse datasets to balance styles beyond manga. It evaluates models in zero-shot and fine-tuning settings, revealing a gap between human and model performance, and sets a new standard for comprehensive comic understanding.
      </p>
    </td>
  </tr>


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/24-MANPU.png' width=100%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://github.com/emanuelevivoli/cdf">
          <span class="papertitle">Comics Datasets Framework: Mix of Comics datasets for detection benchmarking</span>
        </a>
        <br>
        <strong>Emanuele Vivoli</strong>,
        <a href="https://emanuelevivoli.github.io">Irene Campaioli</a>,
        <a href="https://emanuelevivoli.github.io">Mariateresa Nardoni</a>,
        <a href="https://emanuelevivoli.github.io">Niccolò Biondi</a>,
        <a href="https://emanuelevivoli.github.io">Marco Bertini</a>,
        <a href="https://emanuelevivoli.github.io">Dimosthenis Karatzas</a>
        <br>
        <em>ICDAR (workshop) MANPU</em>, 2024 &nbsp <font color="red"><strong>(oral)</strong></font>
        <br>
        <a href="https://github.com/emanuelevivoli/cdf">project page</a>
        /
        <a href="https://arxiv.org/pdf/2407.03540">arXiv</a>
        <p></p>
        <p>
        This work introduces the Comics Datasets Framework, standardizing annotations and addressing manga overrepresentation with the Comics100 dataset. It benchmarks detection architectures to tackle challenges like small datasets and inconsistent annotations, aiming to improve object detection and support more complex computational tasks in comics.
        </p>
      </td>
    </tr>
	


    <tr bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/24-ICDAR.png' width=100%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://github.com/joanlafuente/ComicVT5">
          <span class="papertitle">Multimodal Transformer for Comics Text-Cloze</span>
        </a>
        <br>
				<strong>Emanuele Vivoli</strong>*,
				<a href="https://emanuelevivoli.github.io">Joan Lafuente Baeza</a>*,
				<a href="https://emanuelevivoli.github.io">Ernest Valveny Llobet</a>,
				<a href="https://emanuelevivoli.github.io">Dimosthenis Karatzas</a>	
        <br>
        <em>ICDAR</em>, 2024 &nbsp <font color="red"><strong>(oral)</strong></font>
        <br>
        <a href="https://github.com/joanlafuente/ComicVT5">project page</a>
        /
        <a href="https://arxiv.org/html/2403.03719v1">arXiv</a>
        <p></p>
        <p>
        This work introduces a Multimodal-LLM for a comics text-cloze task, improving accuracy by 10% over existing models through a domain-adapted visual encoder and new OCR annotations, and extends the task to a generative format.
        </p>
      </td>
    </tr>


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/23-ACM-DocEng.png' width=100%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://dl.acm.org/doi/pdf/10.1145/3573128.3609351">
          <span class="papertitle">Deep-learning for dysgraphia detection in children handwritings</span>
        </a>
        <br>
        <a href="https://emanuelevivoli.github.io">Andrea Gemelli</a>*,
        <strong>Emanuele Vivoli</strong>*,
        <a href="https://emanuelevivoli.github.io">Simone Marinai</a>,
        <a href="https://emanuelevivoli.github.io">Tamara Zappaterra</a>
        <br>
        <em>ACM DocEng</em>, 2023 &nbsp <font color="red"><strong>(oral)</strong></font>
        <br>
        <a href="https://dl.acm.org/doi/pdf/10.1145/3573128.3609351">paper</a>
        <p></p>
        <p>
        This paper proposes a smart pen and deep learning-based approach for early dysgraphia detection in children, offering a faster and more objective alternative to the traditional BHK test, validated through handwriting samples and expert interviews.
        </p>
      </td>
    </tr>


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/23-IRCDL.png' width=100%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2302.01451">
          <span class="papertitle">CTE: A Dataset for Contextualized Table Extraction</span>
        </a>
        <br>
				<a href="https://emanuelevivoli.github.io">Andrea Gemelli</a>*,
				<strong>Emanuele Vivoli</strong>*,
        <a href="https://emanuelevivoli.github.io">Simone Marinai</a>
        <br>
        <em>IRCDL</em>, 2023
        <br>
        <a href="https://github.com/AILab-UniFI/cte-dataset">project page</a>
        /
        <a href="https://arxiv.org/abs/2302.01451">arXiv</a>
        <p></p>
        <p>
          This paper introduces Contextualized Table Extraction (CTE), a task to extract and structure tables within their document context, supported by a new dataset of 75k annotated pages from scientific papers, combining data from PubTables-1M and PubLayNet.
        </p>
      </td>
    </tr>



    <tr bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/22-ECCVw-TIE.png' width=100%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2209.06730">
          <span class="papertitle">MUST-VQA: MUltilingual Scene-Text VQA</span>
        </a>
        <br>
				
				<strong>Emanuele Vivoli</strong>,
        <a href="https://emanuelevivoli.github.io">Ali Furkan Biten</a>,
        <a href="https://emanuelevivoli.github.io">Andres Mafla</a>,
				<a href="https://emanuelevivoli.github.io">Dimosthenis Karatzas</a>,
				<a href="https://emanuelevivoli.github.io">Lluis Gomez</a>
        <br>
        <em>ECCV (workshop) Text in Everything</em>, 2022
        <br>
        <a href="https://github.com/emanuelevivoli/must-vqa">project page</a>
        /
        <a href="https://arxiv.org/abs/2209.06730">arXiv</a>
        <p></p>
        <p>
        This paper introduces a framework for Multilingual Scene Text Visual Question Answering (MUST-VQA) that handles new languages in a zero-shot setting, evaluates models in IID and zero-shot scenarios, and demonstrates the effectiveness of adapting multilingual language models for the STVQA task.
        </p>
      </td>
    </tr>


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/22-ICPR.png' width="160">
        </div>
      </td>

      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://cat3d.github.io/">
			<span class="papertitle">Graph Neural Networks and Representation Embedding for Table Extraction in PDF Documents</span>
        </a>
        <br>
				<a href="https://emanuelevivoli.github.io">Andrea Gemelli</a>*,
        <strong>Emanuele Vivoli</strong>*,
        <a href="https://emanuelevivoli.github.io">Simone Marinai</a>
        
        <br>
        <em>ICPR</em>, 2022
        <br>
        <a href="https://github.com/AILab-UniFI/GNN-TableExtraction">project page</a>
        /
        <a href="https://arxiv.org/abs/2208.11203">arXiv</a>
        <p></p>
        <p>
				This work addresses the problem of table extraction in scientific papers using Graph Neural Networks with enriched representation embeddings to improve distinction between tables, cells, and headers, evaluated on a dataset combining PubLayNet and PubTables-1M.
        </p>
      </td>
    </tr>


          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Thanks to Jon Barron for the website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
